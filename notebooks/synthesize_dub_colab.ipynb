{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Voice-Cloned Dubbing with Qwen3-TTS\n",
    "\n",
    "This notebook synthesizes dubbed audio from a translated transcript using Qwen3-TTS voice cloning.\n",
    "\n",
    "**Requirements:** GPU runtime (T4 or better)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install uv package manager\n",
    "!curl -LsSf https://astral.sh/uv/install.sh | sh\n",
    "import os\n",
    "os.environ['PATH'] = f\"/root/.local/bin:{os.environ['PATH']}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Clone the repository\n!git clone https://github.com/pherber3/translate-dub.git\n%cd translate-dub\n\n# Clone Qwen3-TTS (dependency)\n!git clone https://github.com/QwenLM/Qwen3-TTS.git"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use CUDA config and sync dependencies\n",
    "!cp pyproject.cuda.toml pyproject.toml\n",
    "!uv sync"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify GPU is available\n",
    "!uv run python -c \"import torch; print(f'CUDA: {torch.cuda.is_available()}, Device: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else None}')\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Model and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "import librosa\n",
    "from IPython.display import Audio, display, HTML\n",
    "\n",
    "from qwen_tts import Qwen3TTSModel\n",
    "\n",
    "# Config\n",
    "TRANSCRIPT_PATH = \"data/longform_audio/french_conversation_example_speaker_clips/french_conversation_example_en.json\"\n",
    "CLIPS_PATH = \"data/longform_audio/french_conversation_example_speaker_clips/clips_metadata.json\"\n",
    "OUTPUT_DIR = Path(\"output/dubbed\")\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "(OUTPUT_DIR / \"segments\").mkdir(exist_ok=True)\n",
    "\n",
    "MODEL_NAME = \"Qwen/Qwen3-TTS-12Hz-1.7B-Base\"\n",
    "DEVICE = \"cuda:0\"\n",
    "TEMPERATURE = 0.7\n",
    "USE_REF_TEXT = True  # ICL mode for better voice matching\n",
    "SAMPLE_RATE = 24000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load transcript and clips metadata\n",
    "with open(TRANSCRIPT_PATH) as f:\n",
    "    transcript = json.load(f)\n",
    "\n",
    "with open(CLIPS_PATH) as f:\n",
    "    clips_meta = json.load(f)\n",
    "\n",
    "segments = transcript['segments']\n",
    "target_lang = transcript.get('translation', {}).get('target_language', 'en')\n",
    "\n",
    "# Map language codes\n",
    "lang_map = {'en': 'English', 'fr': 'French', 'de': 'German', 'es': 'Spanish', 'it': 'Italian', 'pt': 'Portuguese', 'zh': 'Chinese', 'ja': 'Japanese', 'ko': 'Korean', 'ru': 'Russian'}\n",
    "lang_name = lang_map.get(target_lang, 'Auto')\n",
    "\n",
    "# Get best reference clip per speaker\n",
    "speaker_refs = {}\n",
    "for clip in clips_meta['clips']:\n",
    "    speaker = clip['speaker']\n",
    "    if speaker not in speaker_refs or clip['rank'] < speaker_refs[speaker]['rank']:\n",
    "        speaker_refs[speaker] = clip\n",
    "\n",
    "print(f\"Transcript: {len(segments)} segments\")\n",
    "print(f\"Target language: {lang_name}\")\n",
    "print(f\"Speakers: {list(speaker_refs.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load TTS model\n",
    "print(f\"Loading model: {MODEL_NAME}\")\n",
    "load_start = time.perf_counter()\n",
    "\n",
    "tts_model = Qwen3TTSModel.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    device_map=DEVICE,\n",
    "    dtype=torch.bfloat16,\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    ")\n",
    "\n",
    "print(f\"Model loaded in {time.perf_counter() - load_start:.1f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create voice clone prompts for each speaker\n",
    "print(\"Creating voice clone prompts...\")\n",
    "speaker_prompts = {}\n",
    "\n",
    "for speaker, ref in speaker_refs.items():\n",
    "    ref_path = Path(ref['file'])\n",
    "    if not ref_path.exists():\n",
    "        ref_path = Path(CLIPS_PATH).parent / ref_path.name\n",
    "    \n",
    "    ref_text = ref['text'] if USE_REF_TEXT else None\n",
    "    prompt = tts_model.create_voice_clone_prompt(\n",
    "        ref_audio=str(ref_path),\n",
    "        ref_text=ref_text,\n",
    "        x_vector_only_mode=not USE_REF_TEXT,\n",
    "    )\n",
    "    speaker_prompts[speaker] = prompt\n",
    "    \n",
    "    # Play the reference clip\n",
    "    mode = \"ICL\" if USE_REF_TEXT else \"x-vector\"\n",
    "    print(f\"\\nSpeaker {speaker} reference ({mode} mode):\")\n",
    "    print(f\"  Text: {ref['text'][:60]}...\")\n",
    "    display(Audio(str(ref_path)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Synthesize Segments\n",
    "\n",
    "Each segment is generated and played inline so you can listen as it goes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trim_silence(audio, sr, top_db=25):\n",
    "    \"\"\"Trim leading and trailing silence.\"\"\"\n",
    "    trimmed, _ = librosa.effects.trim(audio, top_db=top_db)\n",
    "    return trimmed\n",
    "\n",
    "generated_segments = []\n",
    "synth_start = time.perf_counter()\n",
    "\n",
    "# Optional: limit segments for testing\n",
    "LIMIT = None  # Set to e.g. 5 to only process first 5 segments\n",
    "process_segments = segments[:LIMIT] if LIMIT else segments\n",
    "\n",
    "print(f\"Synthesizing {len(process_segments)} segments...\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, seg in enumerate(process_segments):\n",
    "    text = seg.get('text', '')\n",
    "    original_text = seg.get('original_text', '')\n",
    "    speaker = seg.get('speaker', 0)\n",
    "    \n",
    "    # Skip non-speech markers and empty text\n",
    "    if text.startswith('[') and text.endswith(']'):\n",
    "        print(f\"[{i+1}/{len(process_segments)}] Skipping: {text}\")\n",
    "        continue\n",
    "    if not text.strip():\n",
    "        print(f\"[{i+1}/{len(process_segments)}] Skipping empty segment\")\n",
    "        continue\n",
    "    if speaker not in speaker_prompts:\n",
    "        print(f\"[{i+1}/{len(process_segments)}] Skipping (no ref): Speaker {speaker}\")\n",
    "        continue\n",
    "    \n",
    "    # Generate\n",
    "    seg_start = time.perf_counter()\n",
    "    wavs, sr = tts_model.generate_voice_clone(\n",
    "        text=text,\n",
    "        language=lang_name,\n",
    "        voice_clone_prompt=speaker_prompts[speaker],\n",
    "        temperature=TEMPERATURE,\n",
    "        do_sample=True,\n",
    "        top_k=50,\n",
    "        top_p=1.0,\n",
    "        repetition_penalty=1.05,\n",
    "    )\n",
    "    \n",
    "    audio = trim_silence(wavs[0], SAMPLE_RATE)\n",
    "    seg_time = time.perf_counter() - seg_start\n",
    "    duration = len(audio) / SAMPLE_RATE\n",
    "    \n",
    "    # Save segment\n",
    "    seg_filename = f\"segment_{i:04d}_speaker_{speaker}.wav\"\n",
    "    seg_path = OUTPUT_DIR / \"segments\" / seg_filename\n",
    "    sf.write(str(seg_path), audio, SAMPLE_RATE)\n",
    "    \n",
    "    generated_segments.append({\n",
    "        'index': i,\n",
    "        'speaker': speaker,\n",
    "        'text': text,\n",
    "        'original_text': original_text,\n",
    "        'audio_file': str(seg_path),\n",
    "        'duration': duration,\n",
    "    })\n",
    "    \n",
    "    # Display with playback\n",
    "    display(HTML(f\"<h4>[{i+1}/{len(process_segments)}] Speaker {speaker}</h4>\"))\n",
    "    display(HTML(f\"<b>Original:</b> {original_text[:80]}{'...' if len(original_text) > 80 else ''}\"))\n",
    "    display(HTML(f\"<b>Translated:</b> {text[:80]}{'...' if len(text) > 80 else ''}\"))\n",
    "    display(HTML(f\"<i>{duration:.2f}s audio generated in {seg_time:.2f}s</i>\"))\n",
    "    display(Audio(audio, rate=SAMPLE_RATE))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synth_time = time.perf_counter() - synth_start\n",
    "print(\"=\" * 60)\n",
    "print(f\"Synthesis completed in {synth_time:.1f}s\")\n",
    "print(f\"Generated {len(generated_segments)} segments\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Combine Segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all segments with gaps\n",
    "GAP_SECONDS = 0.3\n",
    "gap_samples = int(GAP_SECONDS * SAMPLE_RATE)\n",
    "gap_audio = np.zeros(gap_samples, dtype=np.float32)\n",
    "\n",
    "combined_parts = []\n",
    "for seg_info in generated_segments:\n",
    "    audio, _ = sf.read(seg_info['audio_file'])\n",
    "    combined_parts.append(audio)\n",
    "    combined_parts.append(gap_audio)\n",
    "\n",
    "# Remove trailing gap\n",
    "if combined_parts:\n",
    "    combined_parts = combined_parts[:-1]\n",
    "\n",
    "combined_audio = np.concatenate(combined_parts) if combined_parts else np.array([])\n",
    "combined_duration = len(combined_audio) / SAMPLE_RATE\n",
    "\n",
    "# Save combined audio\n",
    "combined_path = OUTPUT_DIR / \"combined_dubbed.wav\"\n",
    "sf.write(str(combined_path), combined_audio, SAMPLE_RATE)\n",
    "\n",
    "print(f\"Combined audio: {combined_path}\")\n",
    "print(f\"Total duration: {combined_duration:.1f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Play the full combined audio\n",
    "display(HTML(\"<h3>Combined Dubbed Audio</h3>\"))\n",
    "display(Audio(combined_audio, rate=SAMPLE_RATE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save metadata\n",
    "metadata = {\n",
    "    'target_language': target_lang,\n",
    "    'model': MODEL_NAME,\n",
    "    'sample_rate': SAMPLE_RATE,\n",
    "    'gap_seconds': GAP_SECONDS,\n",
    "    'synth_seconds': round(synth_time, 2),\n",
    "    'combined_duration': round(combined_duration, 2),\n",
    "    'segments': generated_segments,\n",
    "}\n",
    "\n",
    "metadata_path = OUTPUT_DIR / \"dub_metadata.json\"\n",
    "with open(metadata_path, 'w') as f:\n",
    "    json.dump(metadata, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"Metadata saved: {metadata_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Download Results\n",
    "\n",
    "Download the combined audio and individual segments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "\n",
    "# Download combined audio\n",
    "files.download(str(combined_path))\n",
    "\n",
    "# Optionally download all segments as zip\n",
    "# !zip -r output/segments.zip output/dubbed/segments/\n",
    "# files.download('output/segments.zip')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}